{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from joblib import dump, load\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "# sklearn imports for preprocessing and model building\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    LabelEncoder,\n",
    "    OneHotEncoder,\n",
    "    PolynomialFeatures,\n",
    "    StandardScaler,\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tabulate import tabulate\n",
    "\n",
    "sys.path.insert(0, \"../utils\")\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit                object\n",
      "user_id             int64\n",
      "bust size          object\n",
      "item_id             int64\n",
      "weight            float64\n",
      "rating              int64\n",
      "rented for         object\n",
      "review_text        object\n",
      "body type          object\n",
      "review_summary     object\n",
      "category           object\n",
      "height            float64\n",
      "size                int64\n",
      "age               float64\n",
      "review_date        object\n",
      "review_length       int64\n",
      "band_size         float64\n",
      "cup_size           object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Import csv to df\n",
    "df = pd.read_csv(\"../data/renttherunway_cleaned_data.csv\")\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results dataframe\n",
    "results = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"model\",\n",
    "        \"accuracy\",\n",
    "        \"precision\",\n",
    "        \"recall\",\n",
    "        \"f1\",\n",
    "        \"mse\",\n",
    "        \"ber\",\n",
    "        \"mae\",\n",
    "        \"best_params\",\n",
    "        \"best_estimator\",\n",
    "        \"feature_importances\",\n",
    "        \"confusion_matrix\",\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_transformed_data(data, data_file='../data/transformed_data.joblib', preprocessor_file='../data/preprocessor.joblib'):\n",
    "    # Define categorical and numeric columns\n",
    "    categorical_cols = [\"fit\", \"bust size\", \"rented for\", \"body type\", \"category\", \"cup_size\"]\n",
    "    numeric_cols = [\"weight\", \"height\", \"size\", \"age\", \"review_length\", \"band_size\"]\n",
    "\n",
    "    # Preprocessor for categorical and numeric data\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', SimpleImputer(strategy='mean'), numeric_cols),\n",
    "            ('cat', OneHotEncoder(), categorical_cols)\n",
    "        ], sparse_threshold=0)\n",
    "\n",
    "    # Convert 'rating' using LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    data['rating'] = label_encoder.fit_transform(data['rating'])\n",
    "\n",
    "    # Splitting features and target\n",
    "    X = data.drop(\"rating\", axis=1)\n",
    "    y = data[\"rating\"]\n",
    "\n",
    "    # Applying transformations\n",
    "    X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "    # Saving the transformed data\n",
    "    # dump((X_transformed, y), file_name)\n",
    "    # print(f\"Transformed data saved to {file_name}\")\n",
    "    dump((X_transformed, y), data_file)\n",
    "    dump(preprocessor, preprocessor_file)\n",
    "    print(f\"Transformed data saved to {data_file}\")\n",
    "    print(f\"Preprocessor saved to {preprocessor_file}\")\n",
    "\n",
    "    return X_transformed, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def get_feature_names(column_transformer):\n",
    "    \"\"\"\n",
    "    Get feature names from a ColumnTransformer.\n",
    "    \"\"\"\n",
    "    output_features = []\n",
    "\n",
    "    for transformer in column_transformer.transformers_:\n",
    "        transformer_name, transformer_obj, transformer_cols = transformer\n",
    "\n",
    "        if transformer_name == 'remainder':\n",
    "            # This case handles the remainder columns (those not explicitly transformed)\n",
    "            output_features.extend(transformer_cols)\n",
    "        elif hasattr(transformer_obj, 'get_feature_names_out'):\n",
    "            # This case handles transformers with a `get_feature_names_out` method\n",
    "            transformer_features = transformer_obj.get_feature_names_out(transformer_cols)\n",
    "            output_features.extend(transformer_features)\n",
    "        else:\n",
    "            # This case handles all other transformers\n",
    "            output_features.extend(transformer_cols)\n",
    "\n",
    "    return output_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_model(X_train, y_train, X_test, y_test, preprocessor, model_class, param_grid, full_grid_search=False):\n",
    "    # Create a pipeline with model\n",
    "    model_pipeline = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler(with_mean=False)),\n",
    "        ('classifier', model_class())\n",
    "    ])\n",
    "\n",
    "    # Grid Search with Cross-Validation\n",
    "    if full_grid_search:\n",
    "        grid_search = GridSearchCV(model_pipeline, param_grid, cv=5, scoring=\"accuracy\", verbose=3)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        best_model = grid_search.best_estimator_\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"Best cross-validation score: {grid_search.best_score_}\")\n",
    "    else:\n",
    "        # Use param grid as kwargs for model\n",
    "        best_model = model_pipeline\n",
    "        best_model.set_params(**{'classifier__'+k: v for k, v in param_grid.items()})\n",
    "        best_model.fit(X_train, y_train)\n",
    "\n",
    "        \n",
    "\n",
    "    # Evaluation\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    mse = np.mean((y_test - y_pred) ** 2)\n",
    "    mae = np.mean(np.abs(y_test - y_pred))\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "    recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    ber = 1 - accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)\n",
    "    \n",
    "    # Feature Importances Calculation\n",
    "    try:\n",
    "        feature_names = get_feature_names(preprocessor)\n",
    "\n",
    "        if hasattr(best_model.named_steps['classifier'], 'coef_'):\n",
    "            importances = best_model.named_steps['classifier'].coef_[0]\n",
    "        elif hasattr(best_model.named_steps['classifier'], 'feature_importances_'):\n",
    "            importances = best_model.named_steps['classifier'].feature_importances_\n",
    "        else:\n",
    "            importances = 'N/A'\n",
    "\n",
    "        if importances != 'N/A':\n",
    "            # Map feature names to their importances\n",
    "            feature_importances = {feature: imp for feature, imp in zip(feature_names, importances)}\n",
    "        else:\n",
    "            feature_importances = 'N/A'\n",
    "    except AttributeError:\n",
    "        feature_importances = 'N/A'\n",
    "\n",
    "    # Dynamically aggreagate label importances\n",
    "    # Define a dictionary to store aggregated importances by category\n",
    "    aggregated_importances = {}\n",
    "\n",
    "    # Iterate through feature importances and aggregate importances by category\n",
    "    for feature, importance in feature_importances.items():\n",
    "        # Split the feature name by '_' to get category and label\n",
    "        parts = feature.split('_')\n",
    "        if len(parts) > 1:\n",
    "            category = parts[0]  # Get the category part\n",
    "            if category not in aggregated_importances:\n",
    "                aggregated_importances[category] = 0\n",
    "            # Aggregate the importance within the category\n",
    "            aggregated_importances[category] += importance\n",
    "\n",
    "    # Display the aggregated importances for each category\n",
    "    feature_importances_str = \"\"\n",
    "    for category, importance in aggregated_importances.items():\n",
    "        feature_importances_str += f'{category}: {importance}\\n'\n",
    "\n",
    "    # # Convert feature importances to string\n",
    "    # feature_importances_str = \"\"\n",
    "    # if feature_importances != 'N/A':\n",
    "    #     for key, value in feature_importances.items():\n",
    "    #         feature_importances_str += f\"{key}: {value}\\n\"\n",
    "\n",
    "    run_results = {\n",
    "        \"model\": model_class.__name__,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"mse\": mse,\n",
    "        \"ber\": ber,\n",
    "        \"mae\": mae,\n",
    "        \"best_params\": grid_search.best_params_ if full_grid_search else param_grid,\n",
    "        \"best_estimator\": best_model,\n",
    "        \"feature_importances\": feature_importances_str,\n",
    "        \"confusion_matrix\": confusion_matrix(y_test, y_pred),\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame from the run results\n",
    "    run_results_df = pd.DataFrame([run_results])\n",
    "\n",
    "    # Append to the external results DataFrame\n",
    "    global results  # Ensure we're using the external 'results' DataFrame\n",
    "    results = pd.concat([results, run_results_df], ignore_index=True)\n",
    "\n",
    "    # Print results\n",
    "    print(tabulate(run_results_df, headers=\"keys\", tablefmt=\"psql\"))\n",
    "\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data saved to ../data/transformed_data.joblib\n",
      "Preprocessor saved to ../data/preprocessor.joblib\n"
     ]
    }
   ],
   "source": [
    "# Transform and save data\n",
    "save_transformed_data(df)\n",
    "\n",
    "# Load transformed data and preprocessor\n",
    "X_transformed, y = load('../data/transformed_data.joblib')\n",
    "preprocessor = load('../data/preprocessor.joblib')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[135.  70.   8. ...   0.   0.   0.]\n",
      " [160.  66.  20. ...   0.   0.   0.]\n",
      " [180.  65.  35. ...   0.   0.   1.]\n",
      " [115.  67.   5. ...   0.   0.   0.]\n",
      " [125.  69.   8. ...   0.   0.   0.]]\n",
      "\n",
      "37601     4\n",
      "37009     4\n",
      "165897    3\n",
      "159616    4\n",
      "110372    4\n",
      "Name: rating, dtype: int64\n",
      "\n",
      "[('num', SimpleImputer(), ['weight', 'height', 'size', 'age', 'review_length', 'band_size']), ('cat', OneHotEncoder(), ['fit', 'bust size', 'rented for', 'body type', 'category', 'cup_size']), ('remainder', 'drop', [1, 3, 6, 8, 13])]\n"
     ]
    }
   ],
   "source": [
    "# Print snippet of data\n",
    "print(X_train[:5])\n",
    "print()\n",
    "print(y_train[:5])\n",
    "print()\n",
    "print(preprocessor.transformers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------+-------------+----------+----------+-------+----------+----------+---------------------------------------------------------------------------------+--------------------------------------------------------------------------+-----------------------------------+-----------------------------------+\n",
      "|    | model              |   accuracy |   precision |   recall |       f1 |   mse |      ber |      mae | best_params                                                                     | best_estimator                                                           | feature_importances               | confusion_matrix                  |\n",
      "|----+--------------------+------------+-------------+----------+----------+-------+----------+----------+---------------------------------------------------------------------------------+--------------------------------------------------------------------------+-----------------------------------+-----------------------------------|\n",
      "|  0 | LogisticRegression |   0.557872 |    0.570322 | 0.557872 | 0.532423 |  2.03 | 0.442128 | 0.816675 | {'C': 0.01, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'liblinear'} | Pipeline(steps=[('scaler', StandardScaler(with_mean=False)),             | review: -0.39673819382814185      | [[  144    34    14    23   101]  |\n",
      "|    |                    |            |             |          |          |       |          |          |                                                                                 |                 ('classifier',                                           | band: -0.036210430157179285       |  [  286   138    44    62   330]  |\n",
      "|    |                    |            |             |          |          |       |          |          |                                                                                 |                  LogisticRegression(C=0.01, class_weight='balanced',     | fit: -0.7408724006785893          |  [  710   459   202   275  1500]  |\n",
      "|    |                    |            |             |          |          |       |          |          |                                                                                 |                                     penalty='l1', solver='liblinear'))]) | bust size: -0.0712956840968439    |  [ 2326  1159   620  1299 10688]  |\n",
      "|    |                    |            |             |          |          |       |          |          |                                                                                 |                                                                          | rented for: 0.0054315915974575135 |  [ 3043  1291   809  1754 30428]] |\n",
      "|    |                    |            |             |          |          |       |          |          |                                                                                 |                                                                          | body type: -0.10714417868108135   |                                   |\n",
      "|    |                    |            |             |          |          |       |          |          |                                                                                 |                                                                          | category: -0.5419966005325715     |                                   |\n",
      "|    |                    |            |             |          |          |       |          |          |                                                                                 |                                                                          | cup: -0.1916977645850612          |                                   |\n",
      "+----+--------------------+------------+-------------+----------+----------+-------+----------+----------+---------------------------------------------------------------------------------+--------------------------------------------------------------------------+-----------------------------------+-----------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mcgin\\AppData\\Local\\Temp\\ipykernel_53368\\1379268480.py:45: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if importances != 'N/A':\n"
     ]
    }
   ],
   "source": [
    "lr_param_grid = {\n",
    "    \"C\": [0.01, 0.1, 1, 10, 100],\n",
    "    \"penalty\": [\"l1\", \"l2\"],\n",
    "    \"solver\": [\"saga\", \"liblinear\"],\n",
    "    \"class_weight\": [\"balanced\"],\n",
    "}\n",
    "\n",
    "lr_best_params = {\n",
    "    \"C\": 0.01,\n",
    "    \"class_weight\": \"balanced\",\n",
    "    \"penalty\": \"l1\",\n",
    "    \"solver\": \"liblinear\",\n",
    "}\n",
    "\n",
    "best_lr_model = logistic_regression_model(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    preprocessor,\n",
    "    LogisticRegression,\n",
    "    full_grid_search=False,\n",
    "    param_grid=lr_best_params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"learning_rate\": [0.1, 0.01, 0.001],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"random_state\": [42],\n",
    "}\n",
    "\n",
    "gbc_best_params = {\n",
    "    \"n_estimators\": 300,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"max_depth\": 7,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "best_gbc_model = logistic_regression_model(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    preprocessor,\n",
    "    GradientBoostingClassifier,\n",
    "    full_grid_search=False,\n",
    "    param_grid=gbc_best_params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "Best parameters: {'max_depth': 7, 'n_estimators': 300, 'random_state': 42}\n",
      "Best cross-validation score: 0.6475954358398762\n",
      "+------------------------+------------+-------------+----------+----------+----------+----------+----------+-----------------------------------------------------------+------------------------------------------------------------------------+--------------------------------------+-----------------------------------+\n",
      "| model                  |   accuracy |   precision |   recall |       f1 |      mse |      ber |      mae | best_params                                               | best_estimator                                                         | feature_importances                  | confusion_matrix                  |\n",
      "|------------------------+------------+-------------+----------+----------+----------+----------+----------+-----------------------------------------------------------+------------------------------------------------------------------------+--------------------------------------+-----------------------------------|\n",
      "| RandomForestClassifier |   0.646599 |     0.52435 | 0.646599 | 0.508587 | 0.716414 | 0.353401 | 0.453679 | {'max_depth': 7, 'n_estimators': 300, 'random_state': 42} | RandomForestClassifier(max_depth=7, n_estimators=300, random_state=42) | (fit, 0.5910040570798137)            | [[    0     0     0     4   312]  |\n",
      "|                        |            |             |          |          |          |          |          |                                                           |                                                                        | (user_id, 0.019348137818886186)      |  [    0     0     0     5   855]  |\n",
      "|                        |            |             |          |          |          |          |          |                                                           |                                                                        | (bust size, 0.011969713924491204)    |  [    0     0     0    15  3131]  |\n",
      "|                        |            |             |          |          |          |          |          |                                                           |                                                                        | (item_id, 0.06954389961704768)       |  [    0     0     0    24 16068]  |\n",
      "|                        |            |             |          |          |          |          |          |                                                           |                                                                        | (weight, 0.024050245876110123)       |  [    0     0     0    15 37310]] |\n",
      "|                        |            |             |          |          |          |          |          |                                                           |                                                                        | (rented for, 0.04470289629819364)    |                                   |\n",
      "|                        |            |             |          |          |          |          |          |                                                           |                                                                        | (body type, 0.00818937968018701)     |                                   |\n",
      "|                        |            |             |          |          |          |          |          |                                                           |                                                                        | (category, 0.05377389358242405)      |                                   |\n",
      "|                        |            |             |          |          |          |          |          |                                                           |                                                                        | (height, 0.01157542568663297)        |                                   |\n",
      "|                        |            |             |          |          |          |          |          |                                                           |                                                                        | (size, 0.029741836246752373)         |                                   |\n",
      "|                        |            |             |          |          |          |          |          |                                                           |                                                                        | (age, 0.04630928685095366)           |                                   |\n",
      "|                        |            |             |          |          |          |          |          |                                                           |                                                                        | (review_length, 0.07603294123493606) |                                   |\n",
      "|                        |            |             |          |          |          |          |          |                                                           |                                                                        | (band_size, 0.006382075772992683)    |                                   |\n",
      "|                        |            |             |          |          |          |          |          |                                                           |                                                                        | (cup_size, 0.0073762103305785864)    |                                   |\n",
      "+------------------------+------------+-------------+----------+----------+----------+----------+----------+-----------------------------------------------------------+------------------------------------------------------------------------+--------------------------------------+-----------------------------------+\n"
     ]
    }
   ],
   "source": [
    "rf_param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"random_state\": [42],\n",
    "}\n",
    "\n",
    "rf_best_params = {\n",
    "    \"n_estimators\": 300,\n",
    "    \"max_depth\": 7,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "best_rf_model = logistic_regression_model(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    preprocessor,\n",
    "    RandomForestClassifier,\n",
    "    full_grid_search=False,\n",
    "    param_grid=rf_best_params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    }
   ],
   "source": [
    "svc_param_grid = {\n",
    "    \"C\": [0.1, 1, 10],\n",
    "    \"kernel\": [\"linear\", \"rbf\"],\n",
    "    \"gamma\": [\"scale\", \"auto\"],\n",
    "}\n",
    "\n",
    "svc_best_params = {\n",
    "    \"C\": 1,\n",
    "    \"kernel\": \"rbf\",\n",
    "    \"gamma\": \"scale\",\n",
    "}\n",
    "\n",
    "best_svc_model = logistic_regression_model(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    preprocessor,\n",
    "    SVC,\n",
    "    full_grid_search=False,\n",
    "    param_grid=svc_best_params,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_param_grid = {\n",
    "    \"n_neighbors\": [3, 5, 7],\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "    \"algorithm\": [\"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "}\n",
    "\n",
    "knn_best_params = {\n",
    "    \"n_neighbors\": 5,\n",
    "    \"weights\": \"uniform\",\n",
    "    \"algorithm\": \"ball_tree\",\n",
    "}\n",
    "\n",
    "best_knn_model = logistic_regression_model(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    preprocessor,\n",
    "    KNeighborsClassifier,\n",
    "    full_grid_search=False,\n",
    "    param_grid=knn_best_params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_param_grid = {\n",
    "    \"max_depth\": [3, 5, 7, None],\n",
    "    \"min_samples_split\": [2, 4, 6],\n",
    "    \"min_samples_leaf\": [1, 2, 3],\n",
    "    \"random_state\": [42],\n",
    "}\n",
    "\n",
    "dt_best_params = {\n",
    "    \"max_depth\": 3,\n",
    "    \"min_samples_split\": 2,\n",
    "    \"min_samples_leaf\": 1,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "best_dt_model = logistic_regression_model(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    preprocessor,\n",
    "    DecisionTreeClassifier,\n",
    "    full_grid_search=False,\n",
    "    param_grid=dt_best_params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time in format 20230101-010101\n",
    "now = datetime.datetime.now()\n",
    "time = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Save results df\n",
    "results.to_csv(f\"../results/sklearn_results_{time}.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
